\section{Evaluation}
\label{sec:eval}
In this paper, we attempt to address performance concerns in the switchless architecture: how does the performance compare when running a workload like weather modeling or Apache's Hadoop[CITE]? To this end, we simulate a micro-benchmark with parameters that approximate real workloads on two switchless topologies (torus and cube), and two traditional topologies (fat-tree and hierarchical). As alluded to in Section~\ref{sec:arch}, we do not expect switchless to have higher performance than a fat-tree on all applications. Yet, with the massive force of low-cost attached to switchless, we would hope that any performance degradation would be acceptable.

%We also ran a few more benchmarks such as a bandwidth stress test to see the effect of increasing data transfer rate, as well as the efficiency of UDP vs TCP.
We also ran other simulations to compare the overhead of UDP versus TCP in the data center setting, as well as IP routing versus dimension-ordered routing.

\subsection {Methodology}
For performance evaluation, we use the ns-3 simulator\cite{Ns3:Online}, an open-source discrete-event network simulator developed by the networking research community[CITE]. We modified a significant portion of the simulator for our purpose. These modifications include adding a dimension ordered routing protocol, creating various network topologies for both switchless and conventional architectures, and constructing a highly configurable synthetic micro-benchmark.  Due to time constraints, we did not modify ns-3 to implement wormhole routing and do not make any claims about it in our evaluation.  The ns-3 simulator, augmented with our modifications, models all five OSI stack layers.

As it is hard to define a small representative application for the data center, we used a synthetic micro-benchmark that can be configured to model various different data center application's communication pattern. For example, this micro-benchmark can be configured to generate simple all-to-all style communication traffic with certain intensity. On the other hand, it can also be configured to generate sporadic communications between two random nodes. Note that there are many other possible configurations which create different communication patterns for modeling application communication patterns.

% benchmark configurations (common)
The default configurations for all simulations are detailed in Table~\ref{tab:configurations}, unless noted specifically.  All switchless topologies (torus and cube) use dimension ordered routing, while all traditional topologies (conventional hierarchy and fat-tree) use IP routing.
% # host: 512
% Ethernet links: 1Gbps
% hierarchical: 1 core, balanced fan-outs (agg / edge == edge / host)
% link/switching latency is 500ns for all topologies!!!!!!!!!!! (might be wrong)
% cube z-dimension limit is 40
% all topologies are running on UDP
% switchless are using dimension ordered routing
% 4KB packet length

\subsection {Results}
% Results -- 
% (1) Impact of topology change on performance
% (2) Impact of routing change on performance
% (3) Application Sensitivity (Different application configurations)
% (4) Scalability (host count change)


\subsubsection{N-Body simulation performance}
\newcommand{\nbody}{N-Body}
In a classic \nbody~simulation, all nodes communicate to all other nodes. Figure~\ref{fig:nbody_packetdelay} shows the packet delay distribution of all packets sent in one computation round of an \nbody~simulation for all topologies and network stacks; Figure~\ref{fig:nbody_latency} shows the longest packet delay--the global communication latency of a computation round--compared.

\captionsetup[subfloat]{captionskip=-0.003in}
\begin{figure}
    \centering
    \subfloat[Packet Delay Distribution]
    {
        \includegraphics[width=0.4\textwidth]{Nbody_cube-dimordered_parsed_delay.pdf}
        \label{fig:nbody_packetdelay}
    }
    \\
    \vspace{-0.1in}
    \subfloat[Longest Delay]
    {
        \includegraphics[width=0.4\textwidth]{Nbody_cube-dimordered_parsed_latency.pdf}
        \label{fig:nbody_latency}
    }
    \vspace{-0.07in}
    \caption{N-Body Simulation Results}
    \label{fig:common_topos}
\end{figure}

While switchless topologies have higher occurrences of short packet delay (nodes close together), there are also a smaller number of packets with long delays (nodes far away) that limit the global computation throughput of an \nbody simulation. As shown in Figure~\ref{fig:nbody_latency}, fat-tree is about four times faster than cube and conventional hierarchy, and as much as seven times faster than the torus topology.

On the other hand, it does not necessarily mean that simulation under fat-tree is 4 times faster than cube: the computational time is needed to compute overall performance. [IS THIS TRUE]??
% perhaps we need a plot here, comparing different topologies' time in y axis with computational time in the x axis.

\subsubsection{Weather modeling performance}
In weather modeling, every node communicates with only its neighbors; this means that every node in the system communicates to six other nodes in the cube topology. This workload should map very well to switchless topologies, whereas hierarchical and fat-tree could still perform well but the energy cost may be significant. [WHAT DOES NEIGHBORS MEAN FOR HIERARCHICAL AND FAT-TREE??]

\captionsetup[subfloat]{captionskip=-0.003in}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Weather_cube-dimordered_delay}    
    \caption{Weather Modeling Packet Delay Distribution}
    \label{fig:weather_packetdelay}
\end{figure}

Figure~\ref{fig:weather_packetdelay} shows the distribution graph of all packets transferred in a computation round of a weather modeling simulation with six neighbors. For a reason not quite understood, a long tail can still be seen with both cube and torus topology. [WE SHOULD UNDERSTAND THIS OR GIVE SOME REASONS, I HAVE SOME IDEAS]

On the other hand, there are a few reasons why hierarchical or fat-tree has good performance. The most conspicuous reason is that our micro-benchmark doesn't map exactly to a weather modeling simulation in that for each node we pick the six closest neighbors, which will always be in the same rack [IS THIS TRUE???]. If time allows, we will adjust the parameters for a more accurate results.

\subsubsection{TCP vs UDP}
Data center operators usually run TCP as the level 4 networking stack for better compatibility with MAN and WAN [CITE?? I DONT THINK THIS IS TRUE]. However, TCP has more overhead compared with UDP, and TCP guarantees are not necessary in data centers where dropped packet are rare. As we utilize a full network stack simulator for our study, we implement our topologies with both TCP and UDP and run a simulation exercise to see how the overheads compare.

What we found was that, at least for our simulation set up, TCP has about the same performance as UDP, but packet drops are more frequent, leading to completion time timing out.


\subsubsection{Dimension-ordered vs IP}
As mentioned, for switchless topologies, we had two implementations: one with TCP/IP routing, and the other one with dimension-ordered routing commonly used in NoC for deadlock avoidance. In our simulation, as shown in Figure~\ref{fig:dim_ordered}, we found that for some reason dimension-ordered routing is slower than IP. One possible reason for this is that dimension-ordered protocol restricts the possible data paths that packet can traverse through, and so introduces bandwidth contentions that otherwise might not exist in traditional IP routing.

\captionsetup[subfloat]{captionskip=-0.003in}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{dimension_ordered_udp}
    \caption{Packet latency distribution of a torus network running UDP on top of either IP or dimension-ordered protocol.}
    \label{fig:dim_ordered}
\end{figure}

